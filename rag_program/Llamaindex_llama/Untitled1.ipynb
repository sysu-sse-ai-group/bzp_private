{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adbcee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index.core import Settings, SummaryIndex, load_index_from_storage, StorageContext, Settings, \\\n",
    "    SimpleDirectoryReader, VectorStoreIndex, DocumentSummaryIndex, get_response_synthesizer\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "import time\n",
    "import chromadb\n",
    "\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.retrievers import SummaryIndexLLMRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers import BaseSynthesizer\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.core.tools import RetrieverTool\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ae68d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_time 68.5378475189209\n"
     ]
    }
   ],
   "source": [
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"llama2\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    "    embed_batch_size=100\n",
    ")\n",
    "\n",
    "llm = Settings.llm = Ollama(base_url=\"http://localhost:11434\",model=\"llama2\",)\n",
    "Settings.embed_model = ollama_embedding\n",
    "t1=time.time()\n",
    "documents = SimpleDirectoryReader(\"G:\\data\\Chinese-medical-dialogue-data-master\\Data_statistic\\sample\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "t2=time.time()\n",
    "\n",
    "print(\"index_time\", t2-t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "430312ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>The author does not directly mention art versus engineering in the provided text. However, based on the context, it is possible to make some inferences.\n",
       "\n",
       "The author seems to have a background in computer science and programming, as they mention working on Y Combinator and writing essays about startups. They also mention wanting to paint after their mother's death, which suggests an interest in creative pursuits.\n",
       "\n",
       "On the other hand, the author also mentions being focused on their work and feeling a sense of responsibility to their mother during her illness. This suggests that they may prioritize practical and responsible pursuits over more creative or artistic ones.\n",
       "\n",
       "Overall, it is difficult to say definitively what the author would say about art versus engineering based solely on the provided text. However, it seems likely that they would view engineering as a more practical and responsible field, while reserving a more creative or artistic outlet for their personal interests.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_time1 606.1916863918304 -----------------------------------\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Based on the context of the article, the author's views on art versus engineering are not directly discussed. However, some themes related to creativity, problem-solving, and the intersection of technology and humanities can be gleaned from the text. Here are some possible insights into the author's perspective:\n",
       "\n",
       "1. The author seems to value creativity and problem-solving skills equally with technical expertise. In the passage about Y Combinator, the author highlights Sam Altman's ability to start a startup to make nuclear reactors, indicating that he views entrepreneurship as a creative pursuit alongside engineering.\n",
       "2. The author recognizes the importance of attention and focus in any creative pursuit. When discussing his painting hobby, he notes that he was rusty at first but eventually got back into shape, suggesting that he values the process of dedicating oneself to a creative activity.\n",
       "3. The author seems fascinated by the intersection of technology and humanities, as evidenced by his interest in Lisp and its origins as a model of computation. This curiosity about the foundations of programming languages suggests an appreciation for the interplay between technical and intellectual pursuits.\n",
       "4. The author's personal journey also touches on the idea that there can be value in exploring different creative outlets, even if they seem unrelated to one's main field of expertise. His decision to take up painting after his mother's death and his subsequent engagement with the activity suggest that he finds joy and fulfillment in expressing himself through different forms of creativity.\n",
       "\n",
       "In summary, while the author does not explicitly address art versus engineering, his views on creativity, problem-solving, and the interplay between technology and humanities suggest a nuanced appreciation for the role of both technical and intellectual pursuits in his life.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_time2 606.1916863918304 -----------------------------------\n"
     ]
    }
   ],
   "source": [
    "query_str = \"What would the author say about art vs. engineering?\"\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(query_str)\n",
    "t3=time.time()\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "\n",
    "print(\"query_time1\", t3-t2,\"-----------------------------------\")\n",
    "\n",
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n",
    "response = hyde_query_engine.query(query_str)\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "\n",
    "print(\"query_time2\", t3-t2,\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92a43ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\llamaindex_llama\\Lib\\site-packages\\llama_index\\core\\evaluation\\dataset_generation.py:212: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleDirectoryReader, VectorStoreIndex, Response\n\u001b[0;32m      6\u001b[0m data_generator \u001b[38;5;241m=\u001b[39m DatasetGenerator\u001b[38;5;241m.\u001b[39mfrom_documents(documents)\n\u001b[1;32m----> 8\u001b[0m eval_questions \u001b[38;5;241m=\u001b[39m \u001b[43mdata_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_questions_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m eval_questions\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\llamaindex_llama\\Lib\\site-packages\\llama_index\\core\\evaluation\\dataset_generation.py:328\u001b[0m, in \u001b[0;36mDatasetGenerator.generate_questions_from_nodes\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_questions_from_nodes\u001b[39m(\u001b[38;5;28mself\u001b[39m, num: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    327\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generates questions for each document.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magenerate_questions_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\llamaindex_llama\\Lib\\asyncio\\runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from llama_index.core.evaluation import DatasetGenerator, RelevancyEvaluator\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Response\n",
    "\n",
    "data_generator = DatasetGenerator.from_documents(documents)\n",
    "\n",
    "eval_questions = data_generator.generate_questions_from_nodes()\n",
    "\n",
    "eval_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "989c97e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (579703732.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    E:\\anaconda\\envs\\llamaindex_llama\\python.exe -m pip install spacy\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "E:\\anaconda\\envs\\llamaindex_llama\\python.exe -m pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d267b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex_llama",
   "language": "python",
   "name": "llamaindex_llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
